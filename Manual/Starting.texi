@node Getting started
@chapter Getting started

@menu
* Installation::                           How to install Sherpa
* Running Sherpa::                         How to run the event generator
@end menu

@node Installation
@section Installation

Sherpa is distributed as a tarred and gzipped file named
@code{Sherpa-<version>.tar.gz}, and can be unpacked in the current
working directory with

@example
@code{ tar -zxf Sherpa-<version>.tar.gz} .
@end example

To guarantee successful
installation, the following tools should have been made available on
the system: @code{make}, @code{autoconf}, @code{automake} and
@c update
@code{libtool}. Furthermore, a C++ and FORTRAN compiler must be
provided. Compilation and installation proceed through the following commands

@example
@code{ ./configure}

@code{ make install}
@end example

If not specified differently, the directory structure after installation is organized as follows 

@table @code
@item $(prefix)/bin
Sherpa executeable and scripts

@item $(prefix)/include
headers for process library compilation

@item $(prefix)/lib
basic libraries

@item $(prefix)/share
PDFs, Decaydata, fallback run cards
@end table

The installation directory @code{$(prefix)} can be specified by using the
@code{./configure --prefix /path/to/installation/target} directive and defaults
to the current working directory.

If Sherpa has to be moved to a different directory after the installation,
one has to set the following environment variables for each run:
@itemize @bullet
  @item @code{SHERPA_INCLUDE_PATH=$newprefix/include/SHERPA-MC}
  @item @code{SHERPA_SHARE_PATH=$newprefix/share/SHERPA-MC}
  @item @code{SHERPA_LIBRARY_PATH=$newprefix/lib/SHERPA-MC}
  @item @code{LD_LIBRARY_PATH=$SHERPA_LIBRARY_PATH:$LD_LIBRARY_PATH}
@end itemize

Sherpa can be interfaced with various external packages, e.g. HepMC,
for event output, or LHAPDF, for PDFs. For this to work, the user has
to pass the appropriate commands to the configure step. This is achieved 
as shown below:

@example
@code{./configure --enable-hepmc2=/path/to/hepmc2 --enable-lhapdf=/path/to/lhapdf}
@end example

Here, the paths have to point to the top level installation directories of
the external packages, i.e. the ones containing the @code{lib/}, @code{share/},
... subdirectories.

For a complete list of possible configuration options run
@samp{./configure --help}.

@c If you want to use the built-in interface to Lund fragmentation and hadron
@c decays, you have to compile with Pythia support by specifying the
@c @code{--enable-pythia} option without any argument.

The Sherpa package has successfully been compiled, installed and
tested on SuSE, RedHat / Scientific Linux and Debian / Ubuntu Linux systems 
using the GNU C++ compiler versions 3.2, 3.3, 3.4, 4.0, 4.1, 4.2, 4.3 and 4.4 as
well as on Mac OS X 10 using the GNU C++ compiler version 4.0. In all cases the 
GNU FORTRAN compiler g77 or gfortran has been employed. Note that GCC
version 2.96 is @strong{not} supported.

If you have multiple compilers installed on your system, you can use shell 
environment variables to specify which of these are to be used. A list of 
the available variables is printed with
@example
@code{./configure --help}
@end example
in the Sherpa top level directory and looking at the last lines. Depending 
on the shell you are using, you can set these variables e.g. with export 
(bash) or setenv (csh).
Examples:
@example
@code{export CXX=g++-3.4
export CC=gcc-3.4
export CPP=cpp-3.4}
@end example

@subsubheading MacOS Installation

Installation on MacOS is supported at least in all Sherpa versions > 1.1.2. 
Before that, there might have been problems on the newer MacOS 
versions or architectures (10.5, Intel).
The following issues have come up on Mac installations before, so 
please be aware of them:
@itemize @bullet
@item On 10.4 and 10.5 only gfortran is supported, and you will 
have to install it e.g. from HPC
@item If you want to reconfigure, i.e. run the command @code{autoreconf} 
or @code{(g)libtoolize}, you have to make sure that you have a recent version 
of GNU libtool (>=1.5.22 has been tested). Don’t confuse this with 
the native non-GNU libtool which is installed in @code{/usr/bin/libtool} and 
of no use! Also make sure that your autools (autoconf >= 2.61, 
automake >= 1.10 have been tested) are of recent versions. All this should 
not be necessary though, if you only run @code{configure}.
@item Make sure that you don’t have two versions of g++ and libstdc++ 
installed and being used inconsistently. This appeared e.g. when the gcc 
suite was installed through Fink to get gfortran. This caused Sherpa to 
use the native MacOS compilers but link the libstdc++ from Fink (which 
is located in /sw/lib). You can find out which libraries are used by Sherpa 
by running @code{otool -L bin/Sherpa}
@end itemize


@ignore
For users who modified parts of Sherpa according to their own needs
or who added own modules and who would like to distribute the code,
the @code{makedist} script is available. Its options can be listed by
running @code{makedist -h}.
@end ignore

@node Running Sherpa
@section Running Sherpa


The @code{Sherpa} executable resides in the directory @code{<prefix>/bin/}
where @code{<prefix>} denotes the path to the Sherpa installation
directory. The way a particular simulation will be accomplished is
defined by several parameters, which can all be listed in a
common file, or data card (Parameters can be
alternatively specified on the command line; more details are given
in @ref{Input structure}). 
This steering file is called @code{Run.dat} and some example setups
(i.e. @code{Run.dat} files) are distributed with the current version
of Sherpa. They can be found in the directory @code{<prefix>/share/SHERPA-MC/Examples/}, 
and descriptions of some of their key features can be found in the section
@ref{Examples}.

@strong{Please note:} It is not in general possible to reuse run 
cards from previous Sherpa versions. Often there are small changes 
in the parameter syntax of the run cards from 
one version to the next. 
These changes are documented in our manuals. In addition, 
always use the newer Hadron.dat and Decaydata directories 
(and reapply any changes which you might have applied to the old ones),
see @ref{HADRONS++}.
 
The very first step in running Sherpa 
is therefore to adjust all parameters to the needs of the
desired simulation. The details for properly doing this are given in
@ref{Parameters}. In this section, the focus is on the main
issues for a successful operation of Sherpa. This is illustrated by
discussing and referring to the parameter settings that come in the run card 
@code{./Examples/Tevatron_WJets/Run.dat}. This is a simple run card
created to show the basics of how to operate Sherpa. @strong{It should be 
stressed that this run-card relies on many of Sherpa's default settings,
and, as such, the user should understand those settings before using it to 
look at physics.} For more information on the settings and parameters in 
Sherpa, see @ref{Parameters}, and for more 
examples see the @ref{Examples} section.

@anchor{Process selection and initialization}
@subsection Process selection and initialization

Central to any Monte Carlo simulation is the choice of the hard
processes that initiate the events. These hard processes are
described by matrix elements. In Sherpa,
the selection of processes happens in the @code{(processes)}
part of the steering file.
Only a few 
@iftex 
$2\to2$ 
@end iftex
@ifnottex 
2->2 
@end ifnottex 
reactions have been hard-coded. They are available in the EXTRA_XS module.
The more usual way to compute matrix elements is to employ one of Sherpa's
automatic tree-level generators, AMEGIC++ and Comix, see @ref{Basic structure}.
If no matrix-element generator is selected, using the @ref{ME_SIGNAL_GENERATOR}
tag, then Sherpa will use whichever generator is capable of calculating the 
process, checking EXTRA_XS first, then Comix and then AMEGIC++. Therefore, 
for some processes, several of the 
options are used. In this example, EXTRA_XS calculates the 2->2 part of the 
process, and Comix calculates the 
2->3,4 parts.

To begin with the example, the Sherpa run has to be
started by changing into the @code{<prefix>/share/SHERPA-MC/Examples/Tevatron_WJets/}
directory and executing

@example
@code{<prefix>/bin/Sherpa} 
@end example

The user may also run from an
arbitrary directory, employing
@code{<prefix>/bin/Sherpa PATH=<prefix>/share/SHERPA-MC/Examples/Tevatron_WJets}. In the example, the
keyword @code{PATH} is specified by an absolute path. It may also be 
specified relative to the current working directory. If it is
not specified at all or it is omitted, the current working directory
is understood.

For good book-keeping, it is highly recommended to reserve different
subdirectories for different simulations as is demonstrated with
the example setups. 

If AMEGIC++ is used, Sherpa requires an initialization run, where 
libraries are written out, then the libraries must be compiled and linked by running
a @code{makelibs} script in the working directory, and then Sherpa is 
run again for the actual cross section integrations and event generation.
For an example of how to run Sherpa using AMEGIC++, see @ref{Running Sherpa with AMEGIC++}.

If the Internal hard-coded cross sections or Comix are used,
and AMEGIC++ is not, an initialization run is not needed, and Sherpa will 
calculate the cross sections and generate events during the first run.

As the cross sections are integrated, the
integration over phase space is optimized to arrive at an
efficient event generation.
Subsequently events are generated if @code{EVENTS} was specified
either at the command line or added to the @code{Run.dat} file in the
@code{(run)} section. 

The generated events are not stored into a file by
default; for details on how to store the events see 
@ref{Event output formats}. Note that the computational effort to
go through this procedure of generating, compiling and integrating the
matrix elements of the hard processes depends on the complexity of the
parton-level final states. For low multiplicities (
@iftex 
$2\to2,3,4$ 
@end iftex 
@ifnottex 
2->2,3,4 
@end ifnottex
), of course, it can be followed instantly.

@anchor{Results directory}
Usually more than one generation run is wanted. As long as the
parameters that affect the matrix-element integration are not changed,
it is advantageous to store the cross sections obtained during the
generation run for later use. This saves CPU time especially for large
final-state multiplicities of the matrix elements. To store the
integration results, a @code{<result>} directory has to be created in
@code{Tevatron_WJets} (Alternatively, the command line option 
@option{-g} can be invoked, see @ref{Command line}). Then utilizing an
extended command line reading

@example
@code{<prefix>/bin/Sherpa RESULT_DIRECTORY=<result>/}
@end example

a generation run can be started and the results of the integration
will be stored in @code{<result>}, see @ref{RESULT_DIRECTORY}. The next time this command line is
used, Sherpa will look for the integration results in @code{<result>}
and read them in. Of course, if corresponding parameters do change,
the cross sections have to be re-evaluated for a valid new generation
run. The new results have to be stored in a new directory or the
@code{<result>} directory may be re-used once it has been emptied.
Basically, most of the parameters listed in the @code{(model)}, @code{(me)} and @code{(selector)}
part of @code{Run.dat} determine the calculation of cross sections.
Standard examples are changing the magnitude of couplings,
renormalization or factorization scales, changing the PDF or
centre-of-mass energy, or, applying different cuts at the parton
level. If unsure whether a re-integration is required, a simple
test is to remove the
@code{RESULT_DIRECTORY} option from the run command and check
whether the new integration numbers (statistically) comply with the
stored ones.

One more remark (or maybe warning) concerning the validity of the
process libraries is in order here: it is absolutely mandatory to
generate new library files, whenever the physics model is altered,
i.e. particles are added or removed and hence new or existing
diagrams may or may not anymore contribute to the same final states.
Also, when particle masses are switched on or off new library files
must be generated (however, masses may be changed between non-zero
values keeping the same process libraries). Old library files cannot
account for such changes, since once generated their functional
structure is fixed. The best thing is to create a new and separate
setup directory. Otherwise the @code{Process} and @code{Result}
directories have to be erased:

@example
@code{rm -rf Process/}     and     @code{rm -rf Result/}
@end example

In either case one has to start over with the whole initialization 
procedure to prepare for the generation of events again.



@subsection The example set-up: W+Jets at Tevatron

The setup (or the @code{Run.dat} file) provided in
@code{./Examples/Tevatron_WJets/} can be considered as a standard
example to illustrate the generation of fully hadronized events in Sherpa.
Such events will include effects from parton showering,
hadronization into primary hadrons and their subsequent decays into
stable hadrons. Moreover, the example chosen here nicely demonstrates
how Sherpa is used in the context of merging matrix elements and
parton showers @mycite{Hoeche2009rj}.
In addition to the aforementioned corrections, this simulation of
inclusive 
@iftex 
$W$\/ 
@end iftex 
@ifnottex 
W 
@end ifnottex 
production (with the 
@iftex 
$W$\/ 
@end iftex 
@ifnottex 
W 
@end ifnottex 
decaying into
@iftex 
$e^-\bar{\nu}_e$ 
@end iftex 
@ifnottex 
electron and anti-electron-neutrino 
@end ifnottex
) will then include higher-order jet corrections
at the tree level. As a result the transverse-momentum distribution of
the 
@iftex 
$W$\/ 
@end iftex
@ifnottex 
W 
@end ifnottex 
boson as measured by the D0 and CDF collaborations at
Tevatron Run I can be well described, see also
@mycite{Krauss2004bs},@mycite{Krauss2005nu},@mycite{Gleisberg2005qq}.

Before event generation, the initialization procedure as described in
@ref{Process selection and initialization} has to be completed. The matrix-element processes
included in the setup are the following:
@iftex
\begin{align*}
  p\bar{p}\;&\to\;{\rm parton parton}\;\to\;e^-\bar{\nu}_e\,,\\
  p\bar{p}\;&\to\;{\rm parton parton}\;\to\;e^-\bar{\nu}_e\;+\;{\rm parton}\,.%,\\
  p\bar{p}\;&\to\;{\rm parton parton}\;\to\;e^-\bar{\nu}_e\;+\;{\rm parton}\;+\;{\rm parton}\,.%,\\
  %p\bar{p}&\to jet \, jet \to  e^- \bar{\nu}_e + jet \, jet \,.
\end{align*}
@end iftex
@ifnottex
@example
  proton anti-proton -> parton parton -> electron anti-electron-neutrino + up to two partons
@end example
@end ifnottex

In the @code{(processes)} part of the steering file this translates into
@verbatim
  Process 93 93 -> 11 -12 93{2}
  Order_EW 2;
  CKKW sqr(30/E_CMS)
  End process;
@end verbatim
The physics model for
these processes is the Standard Model (@option{SM}) which is the default 
setting of the parameter @code{MODEL}, in the @code{(model)} part of 
@code{Run.dat}. Fixing the order of
electroweak couplings to @option{2}, matrix elements of all partonic
subprocesses for 
@iftex 
$W$\/ 
@end iftex 
@ifnottex 
W 
@end ifnottex 
production without any and with up to two extra QCD
parton emissions will be generated.
Proton--antiproton collisions are considered at
beam energies of 900 GeV; under the @code{(beam)} part of the @code{Run.dat}
file, one therefore has @code{BEAM_1=2212}, @code{BEAM_2=-2212} and
@code{BEAM_ENERGY_@{1,2@}=980.0}. The default PDF used by Sherpa is 
CTEQ66. Model parameters and couplings can be set in the
@code{Run.dat} section @code{(model)}, and the way couplings are treated
can be defined under the @code{(me)} category. The QCD radiation matrix elements have to be
regularized to obtain meaningful cross sections. This is achieved by
specifying @samp{CKKW sqr(30/E_CMS)} in the @code{(processes)} part of
@code{Run.dat}. Simultaneously, this tag initiates the ME-PS merging procedure.
To eventually obtain fully hadronized events, the @code{FRAGMENTATION} tag
has been left on it's default setting @option{Ahadic},
which will run Sherpa's cluster hadronization, and the tag
@code{DECAYMODEL} has it's default setting 
@option{Hadrons}, which will run Sherpa's hadron decays.
Additionally corrections owing to photon emissions are taken into
account.

To run this example set-up, use the 
@example
@code{<prefix>/bin/Sherpa} 
@end example
command as descibed in @ref{Running Sherpa}. Sherpa displays some output as it
runs. At the start of the run, Sherpa initializes the relevant model, and displays 
a table of particles, with their @ref{PDG codes} and some properties. It also displays
the @ref{Particle containers}, and their contents. The other relevant parts of 
Sherpa are initialized, including the matrix element generator(s). The Sherpa output will look like:

@smallformat
@verbatim
Initialized the beams Monochromatic*Monochromatic
CTEQ6_Fortran_Interface::CTEQ6_Fortran_Interface(): Init member 400.
PDF set 'cteq6.6m' loaded from 'libCTEQ6Sherpa' for beam 1 (P+).
CTEQ6_Fortran_Interface::CTEQ6_Fortran_Interface(): Init member 400.
PDF set 'cteq6.6m' loaded from 'libCTEQ6Sherpa' for beam 2 (P+b).
Initialized the ISR: (SF)*(SF)
CTEQ6_Fortran_Interface::CTEQ6_Fortran_Interface(): Init member 400.
CTEQ6_Fortran_Interface::CTEQ6_Fortran_Interface(): Init member 400.
Initialize the Standard Model from  / Model.dat
Running_AlphaS::Running_AlphaS() {
  Setting \alpha_s according to PDF
  perturbative order 1
  \alpha_s(M_Z) = 0.118
}
Initialized the Beam_Remnant_Handler.
Initialized the Shower_Handler.
Initialized the Fragmentation_Handler.
+----------------------------------+
|                                  |
|      CCC  OOO  M   M I X   X     |
|     C    O   O MM MM I  X X      |
|     C    O   O M M M I   X       |
|     C    O   O M   M I  X X      |
|      CCC  OOO  M   M I X   X     |
|                                  |
+==================================+
|  Color dressed  Matrix Elements  |
|     http://comix.freacafe.de     |
|   please cite  JHEP12(2008)039   |
+----------------------------------+
Matrix_Element_Handler::BuildProcesses(): Looking for processes . done ( 23252 kB, 0s ).
Matrix_Element_Handler::InitializeProcesses(): Performing tests . done ( 23252 kB, 0s ).
Initialized the Matrix_Element_Handler for the hard processes.
Initialized the Soft_Photon_Handler.
@end verbatim
@end smallformat

Then Sherpa will start to integrate the cross sections. The output will look like:
@smallformat
@verbatim
Process_Group::CalculateTotalXSec(): Calculate xs for '2_2__j__j__e-__nu_eb' (Comix)
Starting the calculation. Lean back and enjoy ... .
1019.05 pb +- ( 52.4212 pb = 5.14411 % ) 5000 ( 5013 -> 99.7 % )
full optimization:  ( 0s elapsed / 15s left )
...
@end verbatim
@end smallformat
The first line here displays the process which is being calculated. In this example,
the integration is for the 2->2 process, parton, parton -> electron, neutrino. The
matrix element generator used is displayed after the process. 
As the integration progresses, summary lines are displayed, like the one 
shown above. The current estimate of the cross section is displayed, 
along with its statistical error estimate. The number of phase space points
calculated is displayed after this (@option{5000} in this example), and the 
efficiency is displayed after that. On the line below, the time elapsed is
shown, and an estimate of the total time till the optimization is complete. 

When the integration is complete, the output will look like:

@smallformat
@verbatim
...
1098.97 pb +- ( 0.373022 pb = 0.0339428 % ) 300000 ( 300020 -> 100 % )
integration time:  ( 13s elapsed / 0s left )
1098.86 pb +- ( 0.366442 pb = 0.0333475 % ) 310000 ( 310020 -> 100 % )
integration time:  ( 13s elapsed / 0s left )
2_2__j__j__e-__nu_eb : 1098.86 pb +- ( 0.366442 pb = 0.0333475 % )  exp. eff: 27.3833 %
  reduce max for 2_2__j__j__e-__nu_eb to 0.518502 ( eps = 0.001 )
@end verbatim
@end smallformat
with the final cross section result and its statistical error displayed.

Sherpa will then move on to integrate the other processes specified in the
run card.

When the integration is complete, the event generation will start. 
As the events are being generated, Sherpa will display a summary line stating
how many events have been generated, and an estimate of how long it will take.
When the event generation is complete, Sherpa's 
output looks like:

@smallformat
@verbatim
...
  Event 10000 ( 122 s total )
In Event_Handler::Finish : Summarizing the run may take some time.
+-----------------------------------------------------+
|                                                     |
|  Total XS is 1139.05 pb +- ( 11.1111 pb = 0.97 % )  |
|                                                     |
+-----------------------------------------------------+
@end verbatim
@end smallformat

A summary of the number of events generated is displayed, with the 
total cross section for the processes. 

The generated events are not stored into a file by
default; for details on how to store the events see 
@ref{Event output formats}.


@subsection Parton-level event generation with Sherpa

Sherpa has its own tree-level matrix-element generators called AMEGIC++ and Comix. 
Furthermore, with the module PHASIC++, sophisticated and
robust tools for phase-space integration are provided. Therefore
Sherpa obviously can be used as a cross-section integrator. Because
of the way Monte Carlo integration is accomplished, this immediately
allows for parton-level event generation. Taking the @code{Tevatron_WJets}
setup, users have to modify just a few settings in @code{Run.dat} and
would arrive at a parton-level generation for the process gluon down-quark to electron 
antineutrino and up-quark, to name an example. When, for instance, the
options ``@code{EVENTS=0 OUTPUT=2}'' are added to the command line,
a pure cross-section integration for that process would be obtained
with the results plus integration errors written to the screen.

For the example, the @code{(processes)} section alters to
@verbatim
  Process : 21 1 -> 11 -12 2
  Order_EW 2
  End process
@end verbatim
and under the assumption to start afresh, the initialization procedure has
to be followed as before.
Picking the same collider environment as in the previous
example there are only two more changes before the @code{Run.dat} file
is ready for the calculation of the hadronic cross section of the
process g d to e- nu_e-bar u at Tevatron Run I and subsequent
parton-level event generation with 
@iftex 
S\scalebox{0.8}{HERPA}\xspace 
@end iftex 
@ifnottex
Sherpa 
@end ifnottex
. These changes read
@code{SHOWER_GENERATOR=None}, to switch off parton
showering, and, @code{FRAGMENTATION=Off}, to do so for the
hadronization effects.








